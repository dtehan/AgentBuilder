# TD_OutlierFilterTransform

## Function Name
**TD_OutlierFilterTransform**

## Description
TD_OutlierFilterTransform applies a trained outlier detection model (created by TD_OutlierFilterFit) to identify and handle outliers in new datasets. This function uses the statistical thresholds and parameters learned during the training phase to consistently detect and treat outliers across multiple datasets.

**Key Characteristics:**
- **Model-Based Application**: Uses pre-trained model from TD_OutlierFilterFit
- **Consistent Treatment**: Applies same outlier detection rules across all datasets
- **Flexible Handling**: Supports deletion, replacement, or flagging of outliers
- **Group-Aware**: Maintains group-specific thresholds when model was trained by groups
- **Multiple Columns**: Can process multiple numeric columns simultaneously
- **Production-Ready**: Designed for ETL pipelines and batch processing

The function preserves the outlier detection method (Percentile, Tukey, or Carling), thresholds, and replacement strategies defined during model training, ensuring consistent data quality treatment across development, testing, and production environments.

## When to Use

### Business Applications

**Data Quality Pipelines:**
- Apply consistent outlier handling across daily/weekly data loads
- Ensure production data matches training data quality standards
- Automate data cleansing in ETL/ELT workflows
- Maintain data integrity across multiple data sources

**Machine Learning Operations (MLOps):**
- Clean scoring datasets using same rules as training data
- Prevent model drift caused by outlier contamination
- Ensure feature distributions match training expectations
- Prepare real-time data for model predictions

**Financial Analytics:**
- Detect fraudulent transactions using learned patterns
- Clean pricing data for revenue analytics
- Identify unusual trading patterns or account activity
- Filter erroneous financial reporting values

**E-commerce and Retail:**
- Clean product pricing for catalog management
- Detect unusual order quantities or amounts
- Filter inventory anomalies for demand forecasting
- Identify pricing errors before publication

**Healthcare and Life Sciences:**
- Clean patient vital signs and lab results
- Detect data entry errors in clinical records
- Filter instrument calibration errors
- Prepare clean datasets for clinical research

**Manufacturing and IoT:**
- Clean sensor data streams for quality control
- Detect equipment malfunctions from sensor readings
- Filter measurement errors in production monitoring
- Prepare time-series data for predictive maintenance

## Syntax

```sql
SELECT * FROM TD_OutlierFilterTransform (
    ON { table_name | view_name | query } AS InputTable PARTITION BY { ANY | column_name [,...] }
    ON { table_name | view_name | query } AS FitTable DIMENSION
    USING
    [ Accumulate ('column_name' [,...]) ]
) AS alias;
```

## Required and Optional Elements

### Required Elements

**InputTable (ON ... AS InputTable):**
- Input table or view containing data to be cleaned
- Must contain all columns used during model training
- Can include additional columns (accumulated with Accumulate clause)
- **PARTITION BY specification:**
  - `PARTITION BY ANY`: Distributes rows randomly for parallel processing (use when model trained without groups)
  - `PARTITION BY column_name`: Partitions by group columns (must match columns used in TD_OutlierFilterFit training)

**FitTable (ON ... AS FitTable DIMENSION):**
- Pre-trained outlier detection model from TD_OutlierFilterFit
- Contains statistical thresholds, method parameters, and replacement values
- Broadcast to all AMPs using DIMENSION keyword
- Must be compatible with InputTable column types

### Optional Elements

**Accumulate:**
- Specifies columns to copy from InputTable to output
- Useful for preserving identifiers, timestamps, and metadata
- Does not affect outlier detection logic
- Format: `Accumulate('column1', 'column2', ...)`
- **Common columns to accumulate:**
  - Primary keys and identifiers
  - Date/timestamp columns
  - Category or group columns
  - Descriptive text fields

## Input Specifications

### InputTable Schema

| Column | Data Type | Description | Required |
|--------|-----------|-------------|----------|
| Target columns | NUMERIC types | Columns to check for outliers (must match FitTable) | Yes |
| Group columns | Any type | Columns used for group-specific thresholds (if used in Fit) | Conditional* |
| Additional columns | Any type | Other columns to accumulate in output | No |

*Required if model was trained with group-specific thresholds (PARTITION BY column_name in Fit)

### FitTable Schema

Generated by TD_OutlierFilterFit, containing:
- Column names and statistical thresholds
- Outlier detection method and parameters
- Replacement values for each column
- Group identifiers (if group-specific model)

**Important:** FitTable structure is determined by TD_OutlierFilterFit training parameters.

## Output Specifications

### Output Table Schema

| Column | Data Type | Description |
|--------|-----------|-------------|
| [Original columns] | Same as input | Target columns with outliers handled according to model |
| [Accumulated columns] | Same as input | Columns specified in Accumulate clause |
| is_outlier_[column] | INTEGER | Flag (1=outlier, 0=normal) for each target column |
| outlier_count | INTEGER | Total number of target columns flagged as outliers in this row |

**Output Behavior by Replacement Method:**
- **delete**: Outlier rows completely removed from output
- **null**: Outlier values replaced with NULL
- **median**: Outlier values replaced with median from training data
- **mean**: Outlier values replaced with mean from training data
- **mode**: Outlier values replaced with mode from training data
- **Custom value**: Outlier values replaced with specified constant

## Code Examples

### Example 1: Basic Outlier Filtering with Deletion
**Business Context:** E-commerce company applying trained price outlier model to clean new product catalog data.

```sql
-- Step 1: Apply outlier model to new products (outliers deleted)
CREATE TABLE new_products_cleaned AS (
    SELECT * FROM TD_OutlierFilterTransform (
        ON new_product_imports AS InputTable PARTITION BY ANY
        ON price_outlier_model AS FitTable DIMENSION
        USING
        Accumulate('product_id', 'product_name', 'category', 'import_date')
    ) AS dt
) WITH DATA;

-- Step 2: Review outlier statistics
SELECT
    COUNT(*) as total_products,
    SUM(CASE WHEN outlier_count > 0 THEN 1 ELSE 0 END) as products_with_outliers,
    AVG(outlier_count) as avg_outliers_per_product,
    SUM(is_outlier_unit_price) as price_outliers_detected
FROM new_products_cleaned;

-- Step 3: Analyze products that had outliers
SELECT
    category,
    COUNT(*) as outlier_products,
    AVG(unit_price) as avg_cleaned_price
FROM new_products_cleaned
WHERE outlier_count > 0
GROUP BY category
ORDER BY outlier_products DESC;
```

**Sample Output:**
```
total_products | products_with_outliers | avg_outliers_per_product | price_outliers_detected
---------------|------------------------|--------------------------|------------------------
         9,847 |                    213 |                     0.02 |                     213

category        | outlier_products | avg_cleaned_price
----------------|------------------|------------------
Electronics     |               87 |            249.99
Home & Garden   |               56 |             89.95
Sports          |               43 |            129.50
Toys            |               27 |             34.99
```

**Business Impact:** Automatically removed 213 products with pricing errors before catalog publication, preventing customer confusion and maintaining pricing integrity.

---

### Example 2: Group-Specific Outlier Handling with Median Replacement
**Business Context:** Retail analytics team cleaning sales data with category-specific outlier thresholds.

```sql
-- Step 1: Train group-specific outlier model (Tukey method, k=1.5)
CREATE TABLE category_outlier_model AS (
    SELECT * FROM TD_OutlierFilterFit (
        ON historical_sales AS InputTable
        OUT TABLE OutputTable(category_outlier_model)
        USING
        TargetColumns('order_quantity', 'unit_price', 'discount_percent')
        GroupByColumns('product_category')
        OutlierMethod('tukey')
        Multiplier(1.5)
        ReplacementValue('median')
        RemoveTail('both')
    ) AS dt
) WITH DATA;

-- Step 2: Apply group-specific model to new sales data
CREATE TABLE daily_sales_cleaned AS (
    SELECT * FROM TD_OutlierFilterTransform (
        ON daily_sales_raw AS InputTable PARTITION BY product_category
        ON category_outlier_model AS FitTable DIMENSION
        USING
        Accumulate('order_id', 'customer_id', 'order_date', 'store_id')
    ) AS dt
) WITH DATA;

-- Step 3: Analyze outlier patterns by category
SELECT
    product_category,
    COUNT(*) as total_orders,
    SUM(is_outlier_order_quantity) as qty_outliers,
    SUM(is_outlier_unit_price) as price_outliers,
    SUM(is_outlier_discount_percent) as discount_outliers,
    CAST(SUM(outlier_count) AS FLOAT) / COUNT(*) as avg_outlier_flags_per_order
FROM daily_sales_cleaned
GROUP BY product_category
ORDER BY avg_outlier_flags_per_order DESC;

-- Step 4: Review replaced values
SELECT
    order_id,
    product_category,
    order_quantity,
    unit_price,
    discount_percent,
    outlier_count,
    CASE
        WHEN is_outlier_order_quantity = 1 THEN 'QTY_REPLACED'
        WHEN is_outlier_unit_price = 1 THEN 'PRICE_REPLACED'
        WHEN is_outlier_discount_percent = 1 THEN 'DISCOUNT_REPLACED'
        ELSE 'NO_OUTLIERS'
    END as outlier_type
FROM daily_sales_cleaned
WHERE outlier_count > 0
ORDER BY outlier_count DESC, order_date
SAMPLE 50;
```

**Sample Output:**
```
product_category | total_orders | qty_outliers | price_outliers | discount_outliers | avg_outlier_flags_per_order
-----------------|--------------|--------------|----------------|-------------------|----------------------------
Electronics      |       15,234 |          127 |             89 |                34 |                        0.016
Furniture        |        8,901 |           98 |            112 |                23 |                        0.026
Clothing         |       23,456 |          156 |             67 |                45 |                        0.011
Food             |       31,789 |           89 |             34 |                12 |                        0.004

order_id | product_category | order_quantity | unit_price | discount_percent | outlier_count | outlier_type
---------|------------------|----------------|------------|------------------|---------------|------------------
ORD45123 | Electronics      |             25 |     349.99 |             12.5 |             1 | QTY_REPLACED
ORD45287 | Furniture        |              3 |     899.50 |             18.0 |             1 | PRICE_REPLACED
ORD45309 | Electronics      |             15 |     249.99 |             35.0 |             1 | DISCOUNT_REPLACED
```

**Business Impact:** Cleaned 250+ outliers per day with category-specific thresholds, replacing extreme values with category medians to preserve data volume while improving forecast accuracy by 8%.

---

### Example 3: Production ETL Pipeline with Outlier Flagging
**Business Context:** Manufacturing company integrating outlier detection into nightly sensor data processing.

```sql
-- Production ETL: Clean sensor data and flag outliers for review
CREATE MULTISET TABLE sensor_data_processed AS (
    SELECT
        dt.*,
        -- Add quality score based on outlier count
        CASE
            WHEN outlier_count = 0 THEN 'CLEAN'
            WHEN outlier_count <= 2 THEN 'WARNING'
            ELSE 'CRITICAL'
        END as data_quality_status,
        -- Calculate percentage of metrics that are outliers
        CAST(outlier_count AS FLOAT) / 5.0 * 100 as outlier_percentage,
        -- Add processing metadata
        CURRENT_TIMESTAMP as processed_timestamp,
        'v2.1_tukey_k1.5' as model_version
    FROM TD_OutlierFilterTransform (
        ON sensor_readings_staging AS InputTable PARTITION BY equipment_id
        ON sensor_outlier_model AS FitTable DIMENSION
        USING
        Accumulate('reading_id', 'equipment_id', 'equipment_type', 'reading_timestamp', 'shift_id')
    ) AS dt
) WITH DATA PRIMARY INDEX (equipment_id, reading_timestamp);

-- Create alerts table for critical outliers
CREATE TABLE outlier_alerts AS (
    SELECT
        equipment_id,
        equipment_type,
        reading_timestamp,
        outlier_count,
        -- Flag which sensors had outliers
        CASE WHEN is_outlier_temperature = 1 THEN 'TEMP ' ELSE '' END ||
        CASE WHEN is_outlier_pressure = 1 THEN 'PRESS ' ELSE '' END ||
        CASE WHEN is_outlier_vibration = 1 THEN 'VIB ' ELSE '' END ||
        CASE WHEN is_outlier_rpm = 1 THEN 'RPM ' ELSE '' END ||
        CASE WHEN is_outlier_power_consumption = 1 THEN 'POWER' ELSE '' END as outlier_sensors,
        CURRENT_TIMESTAMP as alert_timestamp
    FROM sensor_data_processed
    WHERE data_quality_status = 'CRITICAL'
) WITH DATA;

-- Generate quality report
SELECT
    equipment_type,
    data_quality_status,
    COUNT(*) as reading_count,
    AVG(outlier_percentage) as avg_outlier_pct,
    SUM(is_outlier_temperature) as temp_outliers,
    SUM(is_outlier_pressure) as pressure_outliers,
    SUM(is_outlier_vibration) as vibration_outliers,
    SUM(is_outlier_rpm) as rpm_outliers,
    SUM(is_outlier_power_consumption) as power_outliers
FROM sensor_data_processed
GROUP BY equipment_type, data_quality_status
ORDER BY equipment_type, data_quality_status;

-- Identify equipment requiring maintenance
SELECT
    equipment_id,
    equipment_type,
    COUNT(*) as critical_readings,
    outlier_sensors,
    MAX(reading_timestamp) as last_critical_reading
FROM outlier_alerts
GROUP BY equipment_id, equipment_type, outlier_sensors
HAVING COUNT(*) >= 3  -- 3+ critical readings in same batch
ORDER BY critical_readings DESC;
```

**Sample Output:**
```
equipment_type | data_quality_status | reading_count | avg_outlier_pct | temp_outliers | pressure_outliers | vibration_outliers | rpm_outliers | power_outliers
---------------|---------------------|---------------|-----------------|---------------|-------------------|--------------------|--------------|-----------------
Compressor     | CLEAN               |         8,934 |            0.00 |             0 |                 0 |                  0 |            0 |               0
Compressor     | WARNING             |           234 |           25.60 |            23 |                45 |                 67 |           89 |              12
Compressor     | CRITICAL            |            12 |           83.30 |             8 |                10 |                 11 |            9 |               4
Pump           | CLEAN               |        12,456 |            0.00 |             0 |                 0 |                  0 |            0 |               0
Pump           | WARNING             |           189 |           22.40 |            15 |                34 |                 28 |           41 |              19

equipment_id | equipment_type | critical_readings | outlier_sensors           | last_critical_reading
-------------|----------------|-------------------|---------------------------|----------------------
COMP-A-0023  | Compressor     |                 5 | TEMP PRESS VIB            | 2024-01-15 23:45:12
PUMP-B-0087  | Pump           |                 4 | VIB RPM                   | 2024-01-15 23:42:38
COMP-A-0156  | Compressor     |                 3 | TEMP POWER                | 2024-01-15 23:38:55
```

**Business Impact:** Automated daily detection of 200+ sensor outliers, triggered 12 maintenance alerts preventing potential equipment failures, and maintained 97.4% clean data rate across 21,000 daily sensor readings.

---

### Example 4: Financial Fraud Detection with Percentile Method
**Business Context:** Bank applying transaction amount outlier model to detect potential fraud in real-time.

```sql
-- Step 1: Train outlier model on historical legitimate transactions
CREATE TABLE transaction_outlier_model AS (
    SELECT * FROM TD_OutlierFilterFit (
        ON legitimate_transactions_history AS InputTable
        OUT TABLE OutputTable(transaction_outlier_model)
        USING
        TargetColumns('transaction_amount', 'daily_transaction_count', 'withdrawal_amount')
        GroupByColumns('account_type', 'customer_segment')
        OutlierMethod('percentile')
        LowerPercentile(0.005)
        UpperPercentile(0.995)
        ReplacementValue('null')
        RemoveTail('upper')  -- Focus on unusually high values
    ) AS dt
) WITH DATA;

-- Step 2: Score today's transactions for fraud risk
CREATE TABLE transactions_fraud_scored AS (
    SELECT
        dt.*,
        -- Calculate fraud risk score
        CASE
            WHEN outlier_count >= 2 THEN 'HIGH_RISK'
            WHEN outlier_count = 1 THEN 'MEDIUM_RISK'
            ELSE 'LOW_RISK'
        END as fraud_risk_level,
        -- Estimate potential fraud amount
        CASE
            WHEN is_outlier_transaction_amount = 1 THEN transaction_amount
            ELSE 0
        END as potential_fraud_amount
    FROM TD_OutlierFilterTransform (
        ON todays_transactions AS InputTable PARTITION BY account_type, customer_segment
        ON transaction_outlier_model AS FitTable DIMENSION
        USING
        Accumulate('transaction_id', 'account_id', 'customer_id', 'transaction_time', 'merchant_name', 'transaction_type')
    ) AS dt
) WITH DATA;

-- Step 3: Generate fraud alert report
SELECT
    fraud_risk_level,
    COUNT(*) as transaction_count,
    SUM(potential_fraud_amount) as total_flagged_amount,
    AVG(transaction_amount) as avg_transaction_amount,
    COUNT(DISTINCT account_id) as unique_accounts,
    COUNT(DISTINCT customer_id) as unique_customers
FROM transactions_fraud_scored
GROUP BY fraud_risk_level
ORDER BY
    CASE fraud_risk_level
        WHEN 'HIGH_RISK' THEN 1
        WHEN 'MEDIUM_RISK' THEN 2
        ELSE 3
    END;

-- Step 4: High-priority fraud cases for investigation
SELECT
    transaction_id,
    account_id,
    customer_id,
    transaction_time,
    transaction_amount,
    merchant_name,
    account_type,
    customer_segment,
    outlier_count,
    -- Flag specific anomalies
    CASE WHEN is_outlier_transaction_amount = 1 THEN 'UNUSUAL_AMOUNT ' ELSE '' END ||
    CASE WHEN is_outlier_daily_transaction_count = 1 THEN 'HIGH_FREQUENCY ' ELSE '' END ||
    CASE WHEN is_outlier_withdrawal_amount = 1 THEN 'LARGE_WITHDRAWAL' ELSE '' END as fraud_indicators
FROM transactions_fraud_scored
WHERE fraud_risk_level = 'HIGH_RISK'
ORDER BY potential_fraud_amount DESC, transaction_time DESC;
```

**Sample Output:**
```
fraud_risk_level | transaction_count | total_flagged_amount | avg_transaction_amount | unique_accounts | unique_customers
-----------------|-------------------|----------------------|------------------------|-----------------|------------------
HIGH_RISK        |                23 |           487,650.00 |              21,202.17 |              21 |               19
MEDIUM_RISK      |               187 |           234,890.00 |               1,256.36 |             175 |              162
LOW_RISK         |            45,678 |                 0.00 |                 234.58 |          38,234 |           35,891

transaction_id | account_id | customer_id | transaction_time    | transaction_amount | merchant_name        | account_type | customer_segment | outlier_count | fraud_indicators
---------------|------------|-------------|---------------------|--------------------|--------------------|--------------|------------------|---------------|-------------------------
TXN9876543     | ACC123456  | CUST98765   | 2024-01-15 23:47:32 |          95,000.00 | Wire Transfer Svc  | Personal     | Standard         |             2 | UNUSUAL_AMOUNT HIGH_FREQUENCY
TXN9876544     | ACC234567  | CUST87654   | 2024-01-15 23:22:18 |          78,500.00 | ATM Withdrawal     | Personal     | Premium          |             2 | UNUSUAL_AMOUNT LARGE_WITHDRAWAL
TXN9876545     | ACC345678  | CUST76543   | 2024-01-15 22:15:45 |          45,000.00 | Online Transfer    | Business     | Standard         |             1 | UNUSUAL_AMOUNT
```

**Business Impact:** Identified 23 high-risk transactions totaling $487K for immediate investigation, detected 187 medium-risk cases for monitoring, and maintained false positive rate under 0.5% by using percentile-based thresholds calibrated to customer segments.

---

### Example 5: Healthcare Patient Vitals with Carling Method
**Business Context:** Hospital system cleaning patient vital signs data with sample-size-adaptive outlier detection.

```sql
-- Step 1: Train outlier model using Carling method (adapts to sample size)
CREATE TABLE vitals_outlier_model AS (
    SELECT * FROM TD_OutlierFilterFit (
        ON patient_vitals_historical AS InputTable
        OUT TABLE OutputTable(vitals_outlier_model)
        USING
        TargetColumns('heart_rate', 'blood_pressure_systolic', 'blood_pressure_diastolic', 'oxygen_saturation', 'temperature')
        GroupByColumns('age_group', 'patient_condition')
        OutlierMethod('carling')
        ReplacementValue('median')
        RemoveTail('both')
    ) AS dt
) WITH DATA;

-- Step 2: Clean today's vital signs readings
CREATE TABLE vitals_cleaned AS (
    SELECT
        dt.*,
        -- Calculate clinical alert level
        CASE
            WHEN is_outlier_heart_rate = 1 OR is_outlier_oxygen_saturation = 1 THEN 'CRITICAL'
            WHEN is_outlier_blood_pressure_systolic = 1 OR is_outlier_blood_pressure_diastolic = 1 THEN 'WARNING'
            WHEN outlier_count > 0 THEN 'MONITOR'
            ELSE 'NORMAL'
        END as clinical_status,
        -- Time since reading
        CURRENT_TIMESTAMP - reading_timestamp as time_since_reading
    FROM TD_OutlierFilterTransform (
        ON patient_vitals_today AS InputTable PARTITION BY age_group, patient_condition
        ON vitals_outlier_model AS FitTable DIMENSION
        USING
        Accumulate('reading_id', 'patient_id', 'patient_mrn', 'reading_timestamp', 'ward', 'nurse_id')
    ) AS dt
) WITH DATA;

-- Step 3: Generate clinical alert report by ward
SELECT
    ward,
    clinical_status,
    COUNT(*) as reading_count,
    COUNT(DISTINCT patient_id) as patient_count,
    SUM(is_outlier_heart_rate) as hr_outliers,
    SUM(is_outlier_blood_pressure_systolic) as bp_sys_outliers,
    SUM(is_outlier_blood_pressure_diastolic) as bp_dia_outliers,
    SUM(is_outlier_oxygen_saturation) as o2_outliers,
    SUM(is_outlier_temperature) as temp_outliers,
    AVG(EXTRACT(MINUTE FROM time_since_reading)) as avg_minutes_old
FROM vitals_cleaned
GROUP BY ward, clinical_status
ORDER BY
    ward,
    CASE clinical_status
        WHEN 'CRITICAL' THEN 1
        WHEN 'WARNING' THEN 2
        WHEN 'MONITOR' THEN 3
        ELSE 4
    END;

-- Step 4: Critical patient alerts requiring immediate attention
SELECT
    patient_id,
    patient_mrn,
    ward,
    age_group,
    patient_condition,
    reading_timestamp,
    heart_rate,
    blood_pressure_systolic || '/' || blood_pressure_diastolic as blood_pressure,
    oxygen_saturation,
    temperature,
    CASE WHEN is_outlier_heart_rate = 1 THEN 'HR ' ELSE '' END ||
    CASE WHEN is_outlier_blood_pressure_systolic = 1 THEN 'BP_SYS ' ELSE '' END ||
    CASE WHEN is_outlier_blood_pressure_diastolic = 1 THEN 'BP_DIA ' ELSE '' END ||
    CASE WHEN is_outlier_oxygen_saturation = 1 THEN 'O2_SAT ' ELSE '' END ||
    CASE WHEN is_outlier_temperature = 1 THEN 'TEMP' ELSE '' END as critical_vitals,
    nurse_id as assigned_nurse
FROM vitals_cleaned
WHERE clinical_status = 'CRITICAL'
    AND EXTRACT(MINUTE FROM time_since_reading) <= 30  -- Within last 30 minutes
ORDER BY reading_timestamp DESC;
```

**Sample Output:**
```
ward | clinical_status | reading_count | patient_count | hr_outliers | bp_sys_outliers | bp_dia_outliers | o2_outliers | temp_outliers | avg_minutes_old
-----|-----------------|---------------|---------------|-------------|-----------------|-----------------|-------------|---------------|----------------
ICU  | CRITICAL        |            12 |            11 |           7 |               2 |               1 |           8 |             0 |            8.3
ICU  | WARNING         |            28 |            24 |           0 |              15 |              13 |           0 |             5 |           12.7
ICU  | NORMAL          |           189 |            87 |           0 |               0 |               0 |           0 |             0 |           15.2
ER   | CRITICAL        |             8 |             8 |           4 |               1 |               0 |           5 |             1 |            5.8
ER   | WARNING         |            34 |            31 |           0 |              19 |              15 |           0 |             3 |            9.4

patient_id | patient_mrn | ward | age_group | patient_condition | reading_timestamp   | heart_rate | blood_pressure | oxygen_saturation | temperature | critical_vitals  | assigned_nurse
-----------|-------------|------|-----------|-------------------|---------------------|------------|----------------|-------------------|-------------|------------------|-----------------
PAT-45678  | MRN-123456  | ICU  | 65-74     | Post-Surgery      | 2024-01-15 23:52:18 |        142 | 165/98         |                87 |        98.9 | HR O2_SAT        | NURSE-0234
PAT-45679  | MRN-123457  | ICU  | 55-64     | Cardiac           | 2024-01-15 23:48:33 |        158 | 178/105        |                89 |        99.1 | HR BP_SYS O2_SAT | NURSE-0235
PAT-45680  | MRN-123458  | ER   | 75-84     | Respiratory       | 2024-01-15 23:45:12 |        125 | 145/88         |                85 |       101.2 | O2_SAT TEMP      | NURSE-0456
```

**Business Impact:** Processed 2,500+ vital sign readings per shift with age-adaptive outlier thresholds using Carling method, generated 12 critical alerts for immediate clinical intervention, reduced false alarms by 35% compared to fixed thresholds, and replaced outlier values with clinically appropriate medians for trend analysis.

---

### Example 6: Seasonal Sales Data with Multiple Target Columns
**Business Context:** Retail chain cleaning multi-metric sales data with seasonal patterns and different handling strategies per metric.

```sql
-- Step 1: Train seasonal outlier model (adjust for Q4 holiday spikes)
CREATE TABLE seasonal_sales_outlier_model AS (
    SELECT * FROM TD_OutlierFilterFit (
        ON sales_historical_adjusted AS InputTable  -- Pre-seasonally adjusted
        OUT TABLE OutputTable(seasonal_sales_outlier_model)
        USING
        TargetColumns('daily_revenue', 'transaction_count', 'avg_basket_size', 'units_sold', 'margin_percent')
        GroupByColumns('store_region', 'store_size_category')
        OutlierMethod('tukey')
        Multiplier(2.0)  -- Less aggressive for seasonal business
        ReplacementValue('median')
        RemoveTail('both')
    ) AS dt
) WITH DATA;

-- Step 2: Apply to yesterday's sales with seasonal adjustment
CREATE TABLE yesterday_sales_cleaned AS (
    SELECT
        dt.*,
        -- Calculate data quality score (0-100)
        100 - (outlier_count * 20) as quality_score,
        -- Flag stores needing review
        CASE
            WHEN outlier_count >= 3 THEN 'AUDIT_REQUIRED'
            WHEN outlier_count >= 2 THEN 'REVIEW'
            ELSE 'OK'
        END as audit_status
    FROM TD_OutlierFilterTransform (
        ON (
            -- Apply seasonal adjustment before outlier detection
            SELECT
                store_id,
                store_region,
                store_size_category,
                sales_date,
                daily_revenue / seasonal_factor as daily_revenue,
                transaction_count,
                avg_basket_size / seasonal_factor as avg_basket_size,
                units_sold / seasonal_factor as units_sold,
                margin_percent
            FROM yesterday_sales_raw
            INNER JOIN seasonal_factors
                ON yesterday_sales_raw.sales_date = seasonal_factors.date_key
                AND yesterday_sales_raw.store_region = seasonal_factors.region
        ) AS InputTable PARTITION BY store_region, store_size_category
        ON seasonal_sales_outlier_model AS FitTable DIMENSION
        USING
        Accumulate('store_id', 'sales_date')
    ) AS dt
) WITH DATA;

-- Step 3: Multi-metric quality analysis by region
SELECT
    store_region,
    store_size_category,
    audit_status,
    COUNT(*) as store_count,
    AVG(quality_score) as avg_quality_score,
    -- Breakdown by metric
    SUM(is_outlier_daily_revenue) as revenue_outliers,
    SUM(is_outlier_transaction_count) as txn_count_outliers,
    SUM(is_outlier_avg_basket_size) as basket_size_outliers,
    SUM(is_outlier_units_sold) as units_outliers,
    SUM(is_outlier_margin_percent) as margin_outliers,
    -- Before/after comparison
    SUM(daily_revenue) as total_cleaned_revenue
FROM yesterday_sales_cleaned
GROUP BY store_region, store_size_category, audit_status
ORDER BY store_region, store_size_category, audit_status;

-- Step 4: Detailed audit list for stores requiring review
SELECT
    store_id,
    store_region,
    store_size_category,
    sales_date,
    daily_revenue,
    transaction_count,
    avg_basket_size,
    units_sold,
    margin_percent,
    quality_score,
    outlier_count,
    -- Specify which metrics are outliers
    TRIM(
        CASE WHEN is_outlier_daily_revenue = 1 THEN 'REVENUE ' ELSE '' END ||
        CASE WHEN is_outlier_transaction_count = 1 THEN 'TXN_COUNT ' ELSE '' END ||
        CASE WHEN is_outlier_avg_basket_size = 1 THEN 'BASKET_SIZE ' ELSE '' END ||
        CASE WHEN is_outlier_units_sold = 1 THEN 'UNITS ' ELSE '' END ||
        CASE WHEN is_outlier_margin_percent = 1 THEN 'MARGIN' ELSE '' END
    ) as outlier_metrics
FROM yesterday_sales_cleaned
WHERE audit_status IN ('REVIEW', 'AUDIT_REQUIRED')
ORDER BY outlier_count DESC, quality_score ASC;

-- Step 5: Trend comparison - cleaned vs. raw
SELECT
    store_region,
    -- Cleaned metrics (outliers replaced with median)
    SUM(cleaned.daily_revenue) as cleaned_revenue,
    AVG(cleaned.margin_percent) as cleaned_avg_margin,
    -- Raw metrics for comparison
    SUM(raw.daily_revenue) as raw_revenue,
    AVG(raw.margin_percent) as raw_avg_margin,
    -- Impact calculation
    SUM(cleaned.daily_revenue) - SUM(raw.daily_revenue) as revenue_adjustment,
    CAST((SUM(cleaned.daily_revenue) - SUM(raw.daily_revenue)) AS FLOAT) / SUM(raw.daily_revenue) * 100 as adjustment_pct
FROM yesterday_sales_cleaned cleaned
INNER JOIN yesterday_sales_raw raw
    ON cleaned.store_id = raw.store_id
    AND cleaned.sales_date = raw.sales_date
GROUP BY store_region
ORDER BY ABS(adjustment_pct) DESC;
```

**Sample Output:**
```
store_region | store_size_category | audit_status    | store_count | avg_quality_score | revenue_outliers | txn_count_outliers | basket_size_outliers | units_outliers | margin_outliers | total_cleaned_revenue
-------------|---------------------|-----------------|-------------|-------------------|------------------|--------------------|--------------------|----------------|-----------------|----------------------
Northeast    | Large               | OK              |         234 |              98.7 |                0 |                  0 |                    0 |              0 |               0 |            5,678,900
Northeast    | Large               | REVIEW          |          12 |              76.5 |                5 |                  8 |                    3 |              7 |               1 |              287,450
Northeast    | Large               | AUDIT_REQUIRED  |           3 |              52.0 |                2 |                  1 |                    1 |              2 |               3 |               45,230
Southeast    | Medium              | OK              |         567 |              99.1 |                0 |                  0 |                    0 |              0 |               0 |            8,234,560

store_id | store_region | store_size_category | sales_date  | daily_revenue | transaction_count | avg_basket_size | units_sold | margin_percent | quality_score | outlier_count | outlier_metrics
---------|--------------|---------------------|-------------|---------------|-------------------|-----------------|------------|----------------|---------------|---------------|-------------------------
STORE234 | Northeast    | Large               | 2024-01-15  |     34,567.00 |             1,234 |           28.01 |      4,567 |          22.50 |            40 |             3 | REVENUE TXN_COUNT UNITS
STORE567 | Southeast    | Medium              | 2024-01-15  |     28,900.00 |               987 |           29.28 |      3,890 |          18.75 |            40 |             3 | REVENUE BASKET_SIZE MARGIN
STORE890 | West         | Small               | 2024-01-15  |     12,345.00 |               456 |           27.07 |      1,234 |          35.20 |            60 |             2 | UNITS MARGIN

store_region | cleaned_revenue | cleaned_avg_margin | raw_revenue  | raw_avg_margin | revenue_adjustment | adjustment_pct
-------------|-----------------|--------------------|--------------|--------------|--------------------|---------------
Northeast    |    6,011,580.00 |              24.32 | 6,234,890.00 |         24.58 |       -223,310.00 |          -3.58
Southeast    |    8,345,670.00 |              23.87 | 8,298,450.00 |         23.92 |         47,220.00 |           0.57
West         |    4,567,890.00 |              25.12 | 4,623,120.00 |         25.34 |        -55,230.00 |          -1.19
```

**Business Impact:** Cleaned daily sales data for 1,200+ stores across 5 metrics, identified 18 stores requiring data quality audits, adjusted revenue by -3.6% to +0.6% by replacing outliers with regional medians, and maintained 98%+ quality scores while accounting for seasonal variations in holiday shopping patterns.

---

## Common Use Cases

### By Industry

**Financial Services:**
- Credit card fraud detection
- Transaction amount anomaly detection
- Trading volume outlier filtering
- Insurance claim amount validation
- Loan default risk scoring

**Healthcare:**
- Patient vital signs monitoring
- Lab result validation
- Medical imaging quality control
- Clinical trial data cleaning
- Hospital operational metrics

**Retail & E-commerce:**
- Price error detection
- Order quantity validation
- Inventory anomaly detection
- Customer behavior profiling
- Promotional campaign analysis

**Manufacturing:**
- Sensor data quality monitoring
- Production defect detection
- Equipment performance tracking
- Supply chain optimization
- Quality control automation

**Telecommunications:**
- Network traffic anomaly detection
- Customer usage pattern analysis
- Billing error detection
- Service quality monitoring
- Churn prediction data preparation

**Energy & Utilities:**
- Smart meter data validation
- Power consumption forecasting
- Equipment failure prediction
- Grid stability monitoring
- Customer usage profiling

### By Analytics Task

**Data Preparation:**
- ML feature engineering
- Training data cleaning
- Test dataset preparation
- Cross-validation splits

**Production Scoring:**
- Real-time fraud detection
- Batch prediction pipelines
- Model serving infrastructure
- A/B testing data preparation

**Data Quality:**
- ETL pipeline validation
- Data governance compliance
- Quality scorecard generation
- Anomaly detection systems

**Business Intelligence:**
- Report data cleaning
- Dashboard data preparation
- KPI calculation inputs
- Executive summary metrics

## Best Practices

### Model Application Strategy

**1. Ensure Model-Data Compatibility:**
- Input data must contain all target columns from training
- Column data types must match FitTable expectations
- If model trained with groups, partition by same columns
- Verify seasonal adjustments match training preprocessing

**2. Monitor Outlier Rates:**
```sql
-- Track outlier rates over time
SELECT
    CAST(processing_date AS DATE) as date,
    COUNT(*) as total_records,
    SUM(outlier_count) as total_outlier_flags,
    CAST(SUM(outlier_count) AS FLOAT) / COUNT(*) as avg_outliers_per_record,
    CAST(SUM(CASE WHEN outlier_count > 0 THEN 1 ELSE 0 END) AS FLOAT) / COUNT(*) * 100 as pct_records_with_outliers
FROM processed_data_history
GROUP BY 1
ORDER BY 1 DESC;
```
- Sudden spikes in outlier rates indicate data distribution shifts
- Model may need retraining if outlier rates exceed 5-10%

**3. Handle Group-Specific Processing:**
- Always use `PARTITION BY group_columns` matching training
- Verify all group combinations exist in FitTable
- Handle new groups not seen during training (requires fallback logic)

**4. Version Control Models:**
- Tag FitTable with model version and training date
- Maintain model lineage and retraining schedules
- Document threshold changes and business justification

### Production Implementation

**1. Build Comprehensive Pipelines:**
```sql
-- Complete production pipeline template
CREATE TABLE production_clean AS (
    SELECT
        dt.*,
        CURRENT_TIMESTAMP as processed_timestamp,
        'model_v2.3_20240115' as model_version,
        CASE
            WHEN outlier_count = 0 THEN 'CLEAN'
            WHEN outlier_count <= 2 THEN 'WARNING'
            ELSE 'CRITICAL'
        END as quality_status
    FROM TD_OutlierFilterTransform (
        ON source_data AS InputTable PARTITION BY group_col
        ON outlier_model AS FitTable DIMENSION
        USING
        Accumulate('id', 'timestamp', 'metadata')
    ) AS dt
) WITH DATA;

-- Separate critical outliers for review
CREATE TABLE outliers_for_review AS (
    SELECT * FROM production_clean
    WHERE quality_status = 'CRITICAL'
) WITH DATA;

-- Clean data for downstream consumers
CREATE TABLE production_clean_only AS (
    SELECT * FROM production_clean
    WHERE quality_status IN ('CLEAN', 'WARNING')
) WITH DATA;
```

**2. Implement Monitoring:**
- Track outlier detection rates by group/category
- Alert on unexpected outlier rate changes
- Monitor model performance metrics
- Log processing statistics for audit trails

**3. Handle Edge Cases:**
- New groups not in training data
- Missing values in target columns
- Schema changes in source data
- Model staleness and retraining triggers

**4. Document Processing Rules:**
- Outlier handling strategy per column
- Business justification for thresholds
- Escalation procedures for critical outliers
- Data retention policies for flagged records

### Performance Optimization

**1. Leverage Parallelization:**
- Use `PARTITION BY ANY` for non-grouped models (random distribution)
- Use `PARTITION BY group_cols` for group-specific models
- DIMENSION keyword broadcasts small FitTable to all AMPs
- Batch process large datasets in time windows

**2. Minimize Data Movement:**
```sql
-- Good: Filter before transformation
CREATE TABLE recent_clean AS (
    SELECT * FROM TD_OutlierFilterTransform (
        ON (SELECT * FROM source WHERE load_date >= CURRENT_DATE - 7) AS InputTable PARTITION BY ANY
        ON model AS FitTable DIMENSION
        USING Accumulate('id', 'date')
    ) AS dt
) WITH DATA;

-- Avoid: Filter after transformation (processes unnecessary data)
CREATE TABLE recent_clean AS (
    SELECT * FROM TD_OutlierFilterTransform (
        ON source AS InputTable PARTITION BY ANY  -- Processes all data
        ON model AS FitTable DIMENSION
        USING Accumulate('id', 'date')
    ) AS dt
    WHERE load_date >= CURRENT_DATE - 7  -- Filters after processing
) WITH DATA;
```

**3. Optimize Accumulate Clause:**
- Only accumulate columns needed downstream
- Avoid accumulating large text/blob columns
- Consider creating separate metadata lookups

**4. Index Strategy:**
- Create primary index on frequently joined columns
- Index partition columns for group-specific queries
- Add secondary indexes on filter columns

### Data Quality and Validation

**1. Validate Before and After:**
```sql
-- Pre-transformation summary
SELECT
    'BEFORE' as stage,
    COUNT(*) as record_count,
    AVG(target_col) as avg_value,
    STDDEV_SAMP(target_col) as std_dev,
    MIN(target_col) as min_value,
    MAX(target_col) as max_value
FROM source_data;

-- Post-transformation summary
SELECT
    'AFTER' as stage,
    COUNT(*) as record_count,
    AVG(target_col) as avg_value,
    STDDEV_SAMP(target_col) as std_dev,
    MIN(target_col) as min_value,
    MAX(target_col) as max_value
FROM cleaned_data;
```

**2. Profile Outlier Patterns:**
- Analyze which groups have highest outlier rates
- Investigate systematic outlier patterns
- Review replaced values for business plausibility

**3. Create Quality Scorecards:**
- Track cleaning effectiveness metrics
- Compare cleaned vs. raw data distributions
- Monitor false positive/negative rates

**4. Establish Feedback Loops:**
- Review flagged outliers with domain experts
- Adjust thresholds based on business feedback
- Retrain models with validated data
- Document edge cases and exceptions

## Related Functions

### Outlier Detection Workflow
- **TD_OutlierFilterFit**: Train outlier detection model (prerequisite)
- **TD_OutlierFilterTransform**: Apply trained model (this function)

### Data Cleaning Functions
- **TD_SimpleImputeFit**: Train missing value imputation model
- **TD_SimpleImputeTransform**: Apply missing value imputation
- **TD_ZTest**: Statistical outlier detection using Z-scores
- **TD_FTest**: Variance comparison for outlier identification

### Data Exploration
- **TD_UnivariateStatistics**: Calculate descriptive statistics
- **TD_Histogram**: Visualize data distributions
- **TD_BoxPlot**: Identify outliers graphically
- **TD_ColumnSummary**: Generate column-level summaries

### Feature Engineering
- **TD_ScaleFit / TD_ScaleTransform**: Normalize features after outlier handling
- **TD_BinCodeFit / TD_BinCodeTransform**: Bin outlier-filtered data
- **TD_Antiselect**: Remove columns after outlier analysis

## Notes and Limitations

### Important Considerations

**1. Model Compatibility:**
- InputTable must contain all target columns from training
- Column names and data types must match exactly
- Group columns (if used) must match training PARTITION BY
- FitTable cannot be reused if source schema changes

**2. PARTITION BY Requirements:**
- Non-grouped models: Use `PARTITION BY ANY`
- Grouped models: Use `PARTITION BY group_columns` matching training
- Incorrect partitioning causes errors or incorrect results
- All group combinations must exist in FitTable

**3. Outlier Handling Behavior:**
- **delete**: Removes entire row (reduces record count)
- **null**: Replaces outlier values with NULL (preserves row)
- **median/mean/mode**: Replaces with training statistic (preserves row and value)
- **Custom value**: Replaces with specified constant (preserves row)

**4. Performance Considerations:**
- FitTable broadcast to all AMPs (must be small)
- Large InputTables partition across AMPs for parallel processing
- Group-specific processing requires hash redistribution
- Real-time applications may need pre-computed models in memory

**5. Data Distribution Assumptions:**
- **Percentile method**: No distribution assumptions
- **Tukey method**: Assumes approximately symmetric distribution
- **Carling method**: Adapts to sample size, handles skewness better
- Retraining required if data distribution significantly shifts

**6. New Data Handling:**
- New groups not in FitTable cause errors
- Requires fallback strategy for unseen groups
- Consider separate "default" model for new groups
- Monitor for concept drift requiring model retraining

### Technical Constraints

**1. Column Requirements:**
- Target columns must be numeric (INTEGER, FLOAT, DOUBLE, DECIMAL)
- Group columns can be any data type
- Column names are case-sensitive
- NULL values in target columns handled based on replacement strategy

**2. Model Retraining Triggers:**
- Outlier rate increases beyond expected range (>10%)
- New business rules or threshold requirements
- Seasonal pattern changes
- Schema modifications in source data
- Significant data distribution shifts

**3. Error Conditions:**
- Missing target columns in InputTable
- Type mismatch between InputTable and FitTable
- New group values not in FitTable
- Invalid or corrupted FitTable
- Memory constraints with very large group counts

### Best Practices Summary

1. **Always validate compatibility** between InputTable and FitTable schemas
2. **Monitor outlier rates** and investigate sudden changes
3. **Use appropriate PARTITION BY** matching training configuration
4. **Implement quality checks** before and after transformation
5. **Version control models** and document changes
6. **Build comprehensive pipelines** with error handling
7. **Optimize for performance** using proper partitioning and indexing
8. **Document business rules** for outlier thresholds and handling
9. **Establish retraining schedules** based on data drift monitoring
10. **Create feedback loops** with domain experts for threshold tuning

## Version Information

- **Teradata Vantage Version**: 17.20
- **Function Category**: Machine Learning - Data Transformation
- **Documentation Generated**: November 2024
- **Model Type**: Fit-Transform Pattern (requires trained model)
